---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I work at BIGAI as a senior research engineer now in Beijing, advised by [Zilong Zheng (éƒ‘å­éš†)](https://zilongzheng.github.io/). I am now working on diffusion language model, long context and long sequence generation research.

I graduated from Computer Technology, Tsinghua University (æ¸…åå¤§å­¦) with a masterâ€™s degree and from Computer Science and Technology, Beijing Institute and Technology (åŒ—äº¬ç†å·¥å¤§å­¦) with a bachelorâ€™s degree.

I have interned at X-Tech, XiZi, SenseTime<img src='images/sensetime.jpg' style="width: 2em;">, IDEA<img src='images/idea.jpg' style="width: 2em;">, MSRA<img src='images/msra.png' style="width: 4em;">, Deepseek<img src='images/deepseek.png' style="width: 4em;">, and BIGAI<img src='images/bigai.jpg' style="width: 1.5em;">. At X-Tech, I was advised by Yifei Jin (é‡‘é€¸é£) and [Jian Li (æå»º)](https://scholar.google.com/citations?hl=zh-CN&user=zX7i1EkAAAAJ). At IDEA, I was advised by [Hao Wang (ç‹æ˜Š)](https://scholar.google.com/citations?hl=zh-CN&user=KqkE1CUAAAAJ) and [Jiaxing Zhang (å¼ å®¶å…´)](https://scholar.google.com/citations?hl=zh-CN&user=ozXuhOUAAAAJ). At MSRA, I was advised by [Zhihao Fan (èŒƒæ™ºæ˜Š)](https://libertfan.github.io/) and [Yeyun Gong (å®«å¶äº‘)](https://scholar.google.com/citations?user=piUkwMYAAAAJ&hl=en). I have published some papers <a href='https://scholar.google.com/citations?user=yn0GDR4AAAAJ'><img src="https://img.shields.io/endpoint?logo=Google%20Scholar&url=https://cdn.jsdelivr.net/gh/wutong4012/wutong4012.github.io@main/gs_data_shieldsio_1.json&labelColor=f6f6f6&color=9cf&style=flat&label=citations"></a> at the top international AI conferences such as NeurIPS, ICML.


# ğŸ”¥ News
- *2024.09*: &nbsp;ğŸ‰ğŸ‰ Our paper is accepted by NIPS 2024 

# ğŸ“ Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2024</div><img src='images/cream.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[An Efficient Recipe for Long Context Extension via Middle-Focused Positional Encoding](https://openreview.net/forum?id=aNHEqFMS0N)

**Tong Wu**, Yanpeng Zhao, Zilong Zheng

[**Github Page**](https://github.com/wutong4012/CREAM)
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2023</div><img src='images/ar_diffusion.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation](https://proceedings.neurips.cc/paper_files/paper/2023/hash/7d866abba506e5a56335e4644ebe18f9-Abstract-Conference.html)

**Tong Wu^**, Zhihao Fan^, Xiao Liu, Hai-Tao Zheng, Yeyun Gong, yelong shen, Jian Jiao, Juntao Li, zhongyu wei, Jian Guo, Nan Duan, Weizhu Chen

[**Github Page**](https://github.com/wutong4012/AR-Diffusion)
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECAI 2023 Oral</div><img src='images/consis.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Enhancing Text Generation with Cooperative Training](https://www.semanticscholar.org/paper/Enhancing-Text-Generation-with-Cooperative-Training-Wu-Wang/6098dfceda1f9f6d60a20f25a180c4fb70c02c2b)

**Tong Wu^**, Hao Wang^, Zhongshen Zeng, Wei Wang, Hai-Tao Zheng, Jiaxing Zhang

[**Github Page**](https://github.com/wutong4012/Self-Consistent-Learning)
</div>
</div>

- [Text Generation with Diffusion Language Models: A Pre-training Approach with Continuous Paragraph Denoise](https://proceedings.mlr.press/v202/lin23d.html), Zhenghao Lin, Yeyun Gong, Yelong Shen, **Tong Wu**, Zhihao Fan, Chen Lin, Nan Duan, Weizhu Chen, **ICML 2023**
- [DeepSeek LLM: Scaling Open-Source Language Models with Longtermism](https://arxiv.org/abs/2401.02954)

# ğŸ“– Educations
- *2021.09 - 2024.06*, Master, Tsinghua University, Beijing. 
- *2017.08 - 2021.06*, Bachelor, Beijing Institute and Technology, Beijing. 

# ğŸ’» Internships
- *2024.02 - 2024.06*, BIGAI, NLCo, Beijing.
- *2023.07 - 2023.11*, Deepseek, Beijing.
- *2022.11 - 2023.07*, MSRA, NLC, Beijing.
- *2022.04 - 2022.10*, IDEA, CCNL, Shenzhen.
- *2021.10 - 2022.02*, SenseTime, Shenzhen.
- *2021.01 - 2021.06*, Xizi, Beijing.
- *2020.01 - 2020.05*, X-Tech, Beijing.




